{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function in Logistic Regression\n",
    "\n",
    "## Random Variable\n",
    "\n",
    "A coin toss is a random variable. We cannot possibly predict the result of the coin toss with certainty.\n",
    "\n",
    "## 0) What are possible events of one coin toss?\n",
    "\n",
    "- Head\n",
    "- Tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) What is a fair coin?\n",
    "\n",
    "If the probability of observing either event of a coin toss is 50%.\n",
    "\n",
    "$P(Heads) = p = 0.5$<br>\n",
    "$P(Tails) = 1-p = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) What is the probability of observing Heads when you toss a fair coin?\n",
    "\n",
    "P(Kopf) = p = 0.5\n",
    "\n",
    "The coin toss follows a **Bernoulli Distribution**. The **Bernoulli Distribution** is a probability distribution for random variables with two possible events.\n",
    "\n",
    "Heads: y = 1 (like Survived = 1)<br>\n",
    "Tails: y = 0 (like Survived = 0)\n",
    "\n",
    "$$\n",
    "P(y) = \\begin{cases} p &\\mbox{if } y = 1 \\\\\n",
    "   1-p & \\mbox{if } y = 0\n",
    "   \\end{cases}\n",
    "$$\n",
    "\n",
    "or (equivalently)\n",
    "\n",
    "$$\n",
    "P(y) = p^y * (1-p)^{1-y}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) What is the probability of observing the sequence of events (*Heads*, *Heads*) if a coin is tossed twice?\n",
    "\n",
    "$$\n",
    "P(Heads, Heads) = p * p = 0.25\n",
    "$$\n",
    "\n",
    "Equivalently it could be written as\n",
    "\n",
    "$$\n",
    "P(Heads, Heads) = P(Heads) * P(Heads)\n",
    "$$\n",
    "\n",
    "or as\n",
    "\n",
    "$$\n",
    "P(Heads, Heads) = (p^{y_1} * (1-p)^{1-y_1}) * (p^{y_2} * (1-p)^{1-y_2})\n",
    "$$\n",
    "\n",
    "oder \n",
    "\n",
    "$$\n",
    "P(Heads, Heads) = \\prod_{i=1}^2 p^{y_i} * (1-p)^{1-y_i}\n",
    "$$\n",
    "\n",
    "--> **The probability of observing a sequence of events can be written as the product of individual probabilities of bernoulli distributed random variables.**\n",
    "\n",
    "Generally:\n",
    "\n",
    "$$\n",
    "P(y_1,...,y_n) = \\prod_{i=1}^N p^{y_i} * (1-p)^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Let's turn around the question\n",
    "\n",
    "**So far**: If we know *p*, what is the probability of observing a certain sequence of events?\n",
    "\n",
    "**New**: If we observe a certain sequence of events in the real world, what does that tell us about *p*?\n",
    "\n",
    "If we observe the sequence \n",
    "\n",
    "- Heads, Heads, Heads, Tails\n",
    "\n",
    "and we do not know whether our coin is fair or not, what can we say about *p*?\n",
    "\n",
    "- We will never know with certainty!!!\n",
    "- Our best guess of what *p* is, is the value for which P(Heads, Heads, Heads, Tails) is highest.\n",
    "\n",
    "We turn the probability of the sequence occuring under p\n",
    "\n",
    "$$\n",
    "P(Kopf, Kopf, Kopf, Zahl) = \\prod_{i=1}^4 p^y_i * (1-p)^{1-y_i} = p * p * p * (1-p)\n",
    "$$\n",
    "\n",
    "into a likelihood-function of an estimate of *p* - *$\\hat{p}$*\n",
    "\n",
    "$$\n",
    "L(\\hat{p}) = \\prod_{i=1}^4 \\hat{p}^y_i * (1-\\hat{p})^{1-y_i} = \\hat{p} * \\hat{p} * \\hat{p} * (1-\\hat{p})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_likelihood(p_hat, sequence):\n",
    "    '''\n",
    "    Calculates the overall probability of observing a sequence of independent\n",
    "    independent Bernoulli trial.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    \n",
    "    p_hat:    Estimate of p\n",
    "    sequence: Observed sequence of events\n",
    "    '''\n",
    "    return (p_hat**np.array(sequence) * (1-p_hat)**(1-np.array(sequence))).prod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Werte f√ºr p die wir ausprobieren wollen\n",
    "probabilities = np.linspace(0, 1, 9)\n",
    "\n",
    "# Kopf, Kopf, Kopf, Zahl\n",
    "sequence = [1, 1, 1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.0 is 0.0\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.125 is 0.002\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.25 is 0.012\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.375 is 0.033\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.5 is 0.062\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.625 is 0.092\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.75 is 0.105\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 0.875 is 0.084\n",
      "The likelihood of observing the sequence [1, 1, 1, 0] if p is 1.0 is 0.0\n"
     ]
    }
   ],
   "source": [
    "# Find the best value for p\n",
    "for probability in probabilities:\n",
    "    print(f'The likelihood of observing the sequence {sequence} if p is {probability} \\\n",
    "is {round(calculate_likelihood(probability, sequence), 3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Which probability are we interested in in the Titanic project?\n",
    "\n",
    "What are the events?<br>\n",
    "**Events**: Survived, Died\n",
    "\n",
    "P(Survived) = p<br>\n",
    "P(Died) = 1-p\n",
    "\n",
    "## 6) We are more interested in conditional probabilities\n",
    "\n",
    "P(Survived|Age,Sex,Pclass) = p<br>\n",
    "P(Died|Age,Sex,Pclass) = 1-p\n",
    "\n",
    "## 7) The probabilities have to be estimated for all possible combinations of X\n",
    "\n",
    "--> That is what the sigmoid function does (continuous - estimates **for all possible values**).<br>\n",
    "--> The coefficients $b, w_1, w_2,...,w_n$ are estimated such that the likelihood is maximized.\n",
    "\n",
    "$$\n",
    "\\hat{p} = \\frac{1}{1+e^{-(b + w_1*Age + w_2*Sex + w_3*Pclass)}}\n",
    "$$\n",
    "\n",
    "## 8) Likelihood Function\n",
    "\n",
    "$$\n",
    "L(\\hat{p}) = \\prod_{i=1}^N \\hat{p}^{y_i} * (1-\\hat{p})^{1-y_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Log-Likelihood Function\n",
    "\n",
    "Mathematical optimization is easier if a function is additive.\n",
    "\n",
    "- Taking the logarithm turns the product into a sum.\n",
    "- The parameters that maximize a function are the same as the parameters that maximize the logarithm of the function.\n",
    "\n",
    "$$\n",
    "log(L(\\hat{p})) = \\sum_{i=1}^N y_i * log(\\hat{p}) + (1-y_i) * log((1-\\hat{p}))\n",
    "$$\n",
    "\n",
    "## 10) Loss Function\n",
    "\n",
    "The loss function is just the negative log-likelihood function.\n",
    "\n",
    "Maximizing a function is the same as minimizing its negative transformation.\n",
    "\n",
    "$$\n",
    "Loss = -log(L(\\hat{p})) = - \\sum_{i=1}^N y_i * log(\\hat{p}) + (1-y_i) * log((1-\\hat{p}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss_own(p_hat, sequence):\n",
    "    '''\n",
    "    Calculates the overall probability of observing a sequence of independent\n",
    "    independent Bernoulli trial.\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    \n",
    "    p_hat:    Estimate of p\n",
    "    sequence: Observed sequence of events\n",
    "    '''\n",
    "    return - (np.log(p_hat)*np.array(sequence) + np.log((1-p_hat))*(1-np.array(sequence))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you would go through the following steps:\n",
    "* Assign X and y and train_test_split\n",
    "* EDA\n",
    "* Feature Engineering\n",
    "* Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df[[\"Pclass\"]]\n",
    "y= df[\"Survived\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit a model\n",
    "from sklearn.linear_model import LogisticRegression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.84371207]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w: slope\n",
    "m.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.43255005])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b: position at x=0\n",
    "m.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at the log loss:\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7499951 , 0.2500049 ],\n",
       "       [0.35690152, 0.64309848],\n",
       "       [0.7499951 , 0.2500049 ],\n",
       "       ...,\n",
       "       [0.7499951 , 0.2500049 ],\n",
       "       [0.35690152, 0.64309848],\n",
       "       [0.7499951 , 0.2500049 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prediction vs probability prediction:\n",
    "m.predict_proba(X) #these are the probabilities of belonging to a particular class (0,1). The model optimises on this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6085336468537563"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y, m.predict_proba(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define sigmoid func:\n",
    "def sigmoid(x, b, w):\n",
    "    return 1/(1+np.exp(-(b+x*w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the probability of a member of Pclass 1 surviving:\n",
    "b= m.intercept_[0]\n",
    "w= m.coef_[0]\n",
    "Pclass = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64309848])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(Pclass,b,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
