{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "* Create a simulation to learn\n",
    "* Data isn't iid\n",
    "* Everything you care about can be reduced to a reward scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new branch of DS \n",
    "* Supervised\n",
    "* Unsupervised\n",
    "* Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical achievements in the field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Alpha Go](https://www.youtube.com/watch?v=l7ngy56GY6k)\n",
    "* [DOTA](https://www.youtube.com/watch?v=tfb6aEUMC04)\n",
    "* [Hide and Seek](https://www.youtube.com/watch?v=kopoLzvh5jY)\n",
    "* [Grid world](https://www.youtube.com/watch?v=AMnW-OsOcl8)\n",
    "* [Cartpole](https://youtu.be/XiigTGKZfks?t=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition - from Pavlov to Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Challenge - dog gets food\n",
    "* Dog hears a bell\n",
    "* Dog gets food\n",
    "* After some time, the bell contains some information about the food\n",
    "* **We should 'backfill' reward information back through time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman - dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Components in a RL problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Rewards - Challenge - competition - positive and negative rewards\n",
    "* Policies - Strategies\n",
    "* Environment - environment\n",
    "* Agents - Players - agents\n",
    "* States - the particular setup of the environment at some given time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](SAR.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Markov Decision Process - How is it different from a regular Markov Chain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Markov_Decision_Process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### two extra concepts: \n",
    "* action nodes - We have decision making power in this chain\n",
    "* Reward 'lines' - positive and negative rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The maths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be summarised as : maximize rewards!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More Formally:\n",
    "\n",
    "**RL** is defined as a tuple, containing States, Actions, Transitions, Rewards, and Discounting\n",
    "\n",
    "$RL=\\{S,A,P,R,γ\\}$\n",
    "\n",
    "**S** is a set of all possible states - the State Space\n",
    "\n",
    "$S=\\{s1,s2,...,sn\\}$\n",
    "\n",
    "**A** is a set of all possible actions - the Action Space\n",
    "\n",
    "$A={a1,a2,...,am}$\n",
    "\n",
    "**P** is the probability distribution of entering state **s'** after taking action **a** in state **s**. Action is intentional, but the resulting state is sampled from a distribution\n",
    "\n",
    "$P(s′,r|s,a)$\n",
    "\n",
    "**R** is the reward recieved for taking **a** in **s**\n",
    "\n",
    "$R(a,s)$\n",
    "\n",
    "The Discount Factor is applied to future rewards. It is normally less than 1, though not guaranteed.\n",
    "\n",
    "$γ$\n",
    "\n",
    "**G** - The Total expected Reward (or **Goal**) at time **t** is the cumulated future discounted return, depending on what actions are taken.\n",
    "\n",
    "$Gt=R_{t+1}+γ∗R_{t+2}...+γ_{p−1}∗R_{t+p}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The solution space:\n",
    "#### 3 main branches:\n",
    "* Value based\n",
    "    * Predicting value\n",
    "    * Q-learning, DQN\n",
    "* Policy based\n",
    "    * Predicting policy\n",
    "    * Policy Gradient, DPN\n",
    "* Model/Environment based\n",
    "    * Predicting what will happen\n",
    "    * World models / MBMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Sutton and Barto  - RI\n",
    " * Bellman - dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do this!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement an example using OpenAI's Gym\n",
    "* A handy library for learning about RL - https://gym.openai.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the cartpole problem\n",
    "#### First we make an environment in which the agent can be trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we implement the agent-environment loop\n",
    "* Start the process by resetting the environment\n",
    "* And return an initial observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-4f201865416d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "env.reset()\n",
    "env.render()\n",
    "time.sleep(10)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00239385, -0.04845769,  0.03056801, -0.00557219])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can achieve the same thing by taking an action - in this case a  `step` in a given direction, 0 for left and 1 for right\n",
    "* This now contains a tuple of items\n",
    "* The first is the previous observation\n",
    "* We also get a reward value\n",
    "* A boolean to tell us if we're done\n",
    "* And a value we don't use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.step(0)\n",
    "#env.step(1)\n",
    "observation = env.step(1)[0]\n",
    "reward = env.step(1)[1]\n",
    "done = env.step(1)[2]\n",
    "_ = env.step(1)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01426569,  0.40619734,  0.02078697, -0.54930846])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#position of the machine: - :left of screen, + : right of screen,\n",
    "# velocity of machine\n",
    "# angle of pole (-: balancing left, +: balancing right)\n",
    "# rotation of the pole\n",
    "\n",
    "observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are things we care about!!\n",
    "* Rewards  - `reward`\n",
    "* (Policies - Strategies)\n",
    "* Environment - `env`\n",
    "* Agent - `function`\n",
    "* States - `obs[2]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already use the `done` boolean to work out if we can stop the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take one: Lets build an agent that takes random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent():\n",
    "    env.reset()\n",
    "    for i in range(1000):\n",
    "        env.render()\n",
    "        obs, reward, done, _ = env.step(env.action_space.sample()) # take a random action\n",
    "        time.sleep(0.1)\n",
    "        if done:\n",
    "            print(f'We survived {i} steps')\n",
    "            env.reset()\n",
    "            break\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We survived 36 steps\n"
     ]
    }
   ],
   "source": [
    "random_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take two: Build an agent that obvserves the environment and takes according action\n",
    "\n",
    "For example:\n",
    "* If the pole is left, move left\n",
    "* If the pole is right, move right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dumb_rl():\n",
    "    obs = env.reset()\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if obs[2] < 0 :\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        time.sleep(0.1)\n",
    "        env.render()\n",
    "        \n",
    "        if done:\n",
    "            print(f'We survived {i} steps')\n",
    "            env.close()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We survived 33 steps\n"
     ]
    }
   ],
   "source": [
    "dumb_rl()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take three: Use some RL techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifically, we are going to build a policy based RL algorithm\n",
    "* To build policy based RL, sample from a game simulation\n",
    "* Run multiple simulations, infer from the simulations which sets of moves results in the highest reward\n",
    "* This is on-policy - a more inefficient method than Q-learning (we need lots of samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1: Sample from n game simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_simulation_data(env): #training data - plays random game 200 times\n",
    "    number_of_games = 200\n",
    "    last_moves = 25\n",
    "    observations = []\n",
    "    actions = []\n",
    "\n",
    "    for i in range(number_of_games):\n",
    "        game_obs = []\n",
    "        game_acts = []\n",
    "        obs = env.reset()\n",
    "\n",
    "        for j in range(1000):\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            game_obs.append(obs)\n",
    "            game_acts.append(action)\n",
    "\n",
    "            if done:\n",
    "                observations += game_obs[:-(last_moves+1)]\n",
    "                actions += game_acts[1:-last_moves]\n",
    "                break\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "\n",
    "    return observations, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sample_simulation_data(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02692673,  0.20807095,  0.0354263 , -0.30120149],\n",
       "       [ 0.03108814,  0.01246245,  0.02940227,  0.00244032],\n",
       "       [ 0.03133739,  0.20715066,  0.02945107, -0.28082276],\n",
       "       ...,\n",
       "       [ 0.09679914,  0.33396282, -0.01716022, -0.36693268],\n",
       "       [ 0.10347839,  0.13908887, -0.02449888, -0.07970965],\n",
       "       [ 0.10626017, -0.05567348, -0.02609307,  0.20514425]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Train an agent to learn the policy embedded in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "m=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3: Set the agent loose in a live simulation\n",
    "* It will act based on the best policy for a given state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_rl(env, m):\n",
    "    #setup the game\n",
    "    obs = env.reset()\n",
    "\n",
    "    for i in range(1000):\n",
    "        #start to play the game\n",
    "        #model, tell me what to do next please\n",
    "        obs = obs.reshape(-1,4) #X data is the simulation\n",
    "        action = int(m.predict(obs)) #y data is the action we should take\n",
    "\n",
    "        #take an according step\n",
    "        obs,reward,done,_ = env.step(action)\n",
    "        #visusalise my results\n",
    "        env.render()\n",
    "        #print(obs, reward)\n",
    "        time.sleep(0.1)\n",
    "        #find out if i died\n",
    "        if done:\n",
    "            print(f'iterations survived {i}')\n",
    "            env.close()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations survived 222\n"
     ]
    }
   ],
   "source": [
    "smart_rl(env, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
